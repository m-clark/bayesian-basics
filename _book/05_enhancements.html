<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Bayesian Basics" />
<meta property="og:type" content="book" />


<meta property="og:description" content="An introduction to Bayesian data analysis." />
<meta name="github-repo" content="m-clark/docs" />


<meta name="date" content="2017-01-07" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to Bayesian data analysis.">

<title>Bayesian Basics</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="..\css\tufte_bookdown\mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#home"><span style="color:transparent">Home</span></a></li>
<li class="has-sub"><a href="00_preface.html#preface">Preface</a><ul>
<li><a href="00_preface.html#prerequisites">Prerequisites</a></li>
</ul></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li class="has-sub"><a href="01_intro.html#bayesian-probability">Bayesian Probability</a><ul>
<li><a href="01_intro.html#conditional-probability-bayes-theorem">Conditional probability &amp; Bayes theorem</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="02_example.html#a-hands-on-example">A Hands-on Example</a><ul>
<li><a href="02_example.html#prior-likelihood-posterior-distributions">Prior, likelihood, &amp; posterior distributions</a></li>
<li><a href="02_example.html#prior">Prior</a></li>
<li><a href="02_example.html#likelihood">Likelihood</a></li>
<li><a href="02_example.html#posterior">Posterior</a></li>
<li><a href="02_example.html#posterior-predictive">Posterior predictive</a></li>
</ul></li>
<li class="has-sub"><a href="03_models.html#regression-models">Regression Models</a><ul>
<li><a href="03_models.html#example-linear-regression-model">Example: Linear Regression Model</a></li>
<li class="has-sub"><a href="03_models.html#setup">Setup</a><ul>
<li><a href="03_models.html#stan-code">Stan Code</a></li>
<li><a href="03_models.html#running-the-model">Running the Model</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_diagnostics.html#model-checking-diagnostics">Model Checking &amp; Diagnostics</a><ul>
<li class="has-sub"><a href="04_diagnostics.html#monitoring-convergence">Monitoring Convergence</a><ul>
<li><a href="04_diagnostics.html#visual-inspection-traceplot-densities">Visual Inspection: Traceplot &amp; Densities</a></li>
<li><a href="04_diagnostics.html#statistical-measures">Statistical Measures</a></li>
<li><a href="04_diagnostics.html#autocorrelation">Autocorrelation</a></li>
</ul></li>
<li class="has-sub"><a href="04_diagnostics.html#model-checking">Model Checking</a><ul>
<li><a href="04_diagnostics.html#sensitivity-analysis">Sensitivity Analysis</a></li>
<li><a href="04_diagnostics.html#predictive-accuracy-model-comparison">Predictive Accuracy &amp; Model Comparison</a></li>
<li><a href="04_diagnostics.html#posterior-predictive-checking-statistical">Posterior Predictive Checking: Statistical</a></li>
<li><a href="04_diagnostics.html#posterior-predictive-checking-graphical">Posterior Predictive Checking: Graphical</a></li>
</ul></li>
<li><a href="04_diagnostics.html#summary">Summary</a></li>
</ul></li>
<li class="has-sub"><a href="05_enhancements.html#model-enhancements">Model Enhancements</a><ul>
<li><a href="05_enhancements.html#generating-new-variables-of-interest">Generating New Variables of Interest</a></li>
<li><a href="05_enhancements.html#robust-regression">Robust Regression</a></li>
<li><a href="05_enhancements.html#generalized-linear-model">Generalized Linear Model</a></li>
</ul></li>
<li class="has-sub"><a href="06_issues.html#issues">Issues</a><ul>
<li><a href="06_issues.html#debugging">Debugging</a></li>
<li class="has-sub"><a href="06_issues.html#choice-of-prior">Choice of Prior</a><ul>
<li><a href="06_issues.html#noninformative-weakly-informative-informative">Noninformative, Weakly Informative, Informative</a></li>
<li><a href="06_issues.html#conjugacy">Conjugacy</a></li>
<li><a href="06_issues.html#sensitivity-analysis-revisited">Sensitivity Analysis Revisited</a></li>
<li><a href="06_issues.html#summary-1">Summary</a></li>
</ul></li>
<li class="has-sub"><a href="06_issues.html#sampling-procedure">Sampling Procedure</a><ul>
<li><a href="06_issues.html#metropolis">Metropolis</a></li>
<li><a href="06_issues.html#gibbs">Gibbs</a></li>
<li><a href="06_issues.html#hamiltonian-monte-carlo">Hamiltonian Monte Carlo</a></li>
<li><a href="06_issues.html#other-variations-and-approximate-methods">Other Variations and Approximate Methods</a></li>
</ul></li>
<li><a href="06_issues.html#number-of-draws-thinning-warm-up">Number of draws, thinning, warm-up</a></li>
<li><a href="06_issues.html#model-complexity">Model Complexity</a></li>
</ul></li>
<li><a href="1000_Conclusion.html#summary-2">Summary</a></li>
<li class="has-sub"><a href="1001_Appendix.html#appendix">Appendix</a><ul>
<li class="has-sub"><a href="1001_Appendix.html#maximum-likelihood-review">Maximum Likelihood Review</a><ul>
<li><a href="1001_Appendix.html#example">Example</a></li>
</ul></li>
<li><a href="1001_Appendix.html#linear-model">Linear Model</a></li>
<li><a href="1001_Appendix.html#binomial-likelihood-example">Binomial Likelihood Example</a></li>
<li class="has-sub"><a href="1001_Appendix.html#modeling-languages">Modeling Languages</a><ul>
<li><a href="1001_Appendix.html#bugs">Bugs</a></li>
<li><a href="1001_Appendix.html#jags">JAGS</a></li>
<li><a href="1001_Appendix.html#stan">Stan</a></li>
<li><a href="1001_Appendix.html#r">R</a></li>
<li><a href="1001_Appendix.html#general-statistical-packages">General Statistical Packages</a></li>
<li><a href="1001_Appendix.html#other-programming-languages">Other Programming Languages</a></li>
<li><a href="1001_Appendix.html#summary-3">Summary</a></li>
</ul></li>
<li><a href="1001_Appendix.html#bugs-example">BUGS Example</a></li>
<li><a href="1001_Appendix.html#jags-example">JAGS Example</a></li>
<li><a href="1001_Appendix.html#metropolis-hastings-example">Metropolis Hastings Example</a></li>
<li><a href="1001_Appendix.html#hamiltonian-monte-carlo-example">Hamiltonian Monte Carlo Example</a></li>
</ul></li>
<li class="has-sub"><a href="1002_references.html#references">References</a><ul>
<li><a href="1002_references.html#texts">Texts</a></li>
<li><a href="1002_references.html#other">Other</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="model-enhancements" class="section level1">
<h1>Model Enhancements</h1>
<p>Enhancing and making adjustments to a model can often be straightforward in the Bayesian context, depending on what one wants to accomplish. In other cases, some things may be possible that aren’t readily available with standard approaches in the traditional setting. The following shows a few brief examples to give an idea of the possibilities.</p>
<div id="generating-new-variables-of-interest" class="section level2">
<h2>Generating New Variables of Interest</h2>
<p>We have already seen one way to get at new statistics of interest in the predictive model checking section. I next show how to do so as part of the modeling process itself. In Stan we can accomplish this via the generated quantities section.</p>
<p>A typical part of linear regression output is <span class="math inline">\(R^2\)</span>, the amount of variance accounted for by the model. To get this in Stan we just have to create the code necessary for the calculations, and place it within the generated quantities section. I only show this part of the model code; everything we had before would remain the same. For comparison I show the corresponding R code. There are a couple of ways to go about this, and I use some of Stan’s matrix operations as one approach.</p>
<!-- You ran this and saved the image as data/mainModelDatawithRsq.RData -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">.
.
.

generated quantities {
  real rss;                
  real totalss;
  real&lt;lower=<span class="dv">0</span>, upper=<span class="dv">1</span>&gt;<span class="st"> </span>R2;                 
  vector[N] mu;
  
  mu &lt;-<span class="st"> </span>X *<span class="st"> </span>beta;
  rss &lt;-<span class="st"> </span><span class="kw">dot_self</span>(y-mu);
  totalss &lt;-<span class="st"> </span><span class="kw">dot_self</span>(y-<span class="kw">mean</span>(y));
  R2 &lt;-<span class="st"> </span><span class="dv">1</span> -<span class="st"> </span>rss/totalss;
}</code></pre></div>
<p>Using the results from the model using <span class="func">lm</span>, we do the same calculations for <code>rss</code> and <code>totalss</code>, and note the result is identical to what you’d see in the summary of the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span>-rss/totalss; <span class="kw">summary</span>(modlm)$r.squared</code></pre></div>
<pre><code>[1] 0.4524289</code></pre>
<pre><code>[1] 0.4524289</code></pre>
<p>Now we can run the model with added <span class="math inline">\(R^2\)</span>. Note that as before we do not just get a point estimate, but a whole distribution of simulated values for <span class="math inline">\(R^2\)</span>. First the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fitRsq, <span class="dt">digits=</span><span class="dv">3</span>, <span class="dt">par=</span><span class="kw">c</span>(<span class="st">&#39;beta&#39;</span>,<span class="st">&#39;sigma&#39;</span>,<span class="st">&#39;R2&#39;</span>), <span class="dt">prob=</span><span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">5</span>,.<span class="dv">975</span>))</code></pre></div>
<pre><code>Inference for Stan model: stanmodelcodeRsq.
3 chains, each with iter=12000; warmup=2000; thin=10; 
post-warmup draws per chain=1000, total post-warmup draws=3000.

          mean se_mean    sd   2.5%    50%  97.5% n_eff  Rhat
beta[1]  4.895   0.002 0.129  4.639  4.897  5.144  3000 1.000
beta[2]  0.087   0.003 0.131 -0.169  0.086  0.342  2751 1.000
beta[3] -1.466   0.002 0.125 -1.712 -1.469 -1.219  2826 0.999
beta[4]  0.821   0.002 0.123  0.584  0.820  1.063  3000 0.999
sigma    2.028   0.002 0.091  1.858  2.025  2.212  2945 1.000
R2       0.443   0.000 0.006  0.427  0.445  0.451  2932 1.000

Samples were drawn using NUTS(diag_e) at Sat May 24 13:10:08 2014.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>The nice thing here is that our <span class="math inline">\(R^2\)</span> incorporates the additional uncertainty in estimating the model parameters, and thus acts like an <em>adjusted</em> <span class="math inline">\(R^2\)</span><label for="tufte-sn-27" class="margin-toggle sidenote-number">27</label><input type="checkbox" id="tufte-sn-27" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">27</span> See <a href="http://www.stat.columbia.edu/~gelman/research/published/rsquared.pdf">Gelman &amp; Pardoe (2006)</a>, Bayesian Measures of Explained Variance.</span>. The following is the classical regression adjusted <span class="math inline">\(R^2\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(modlm)$adj</code></pre></div>
<pre><code>[1] 0.4457512</code></pre>
<p>Furthermore, in the Bayesian context we get an interval estimate and everything else we typically get as with other quantities of interest, and the same goes for anything we calculate along the way (e.g. the mu values). In addition, it would be trivial to calculate something like the actual adjusted <span class="math inline">\(R^2\)</span>, the probability that the value is greater than .5, and other things of that nature.</p>
</div>
<div id="robust-regression" class="section level2">
<h2>Robust Regression</h2>
<p>If we were concerned that extreme observations exist that our current model is not able to capture well, we could change the sampling distribution to one that had a little more probability in the tails. This is very easy to do in this situation, as we just change likelihood portion of our code to employ say, a t-distribution. In Stan, the t-distribution has parameters mean and sigma as with the normal distribution, but we also have the added parameter for degrees of freedom. Thus our code might look like the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stanmodelcodeT =<span class="st"> &quot;</span>
<span class="st">.</span>
<span class="st">.</span>
<span class="st">.</span>

<span class="st">model {                     </span>
<span class="st">  vector[N] mu;</span>
<span class="st">  mu &lt;- X * beta;           </span>
<span class="st">  </span>
<span class="st">  // priors</span>
<span class="st">  beta ~ normal(0, 10);</span>
<span class="st">  sigma ~ cauchy(0, 5);     </span>
<span class="st">  </span>
<span class="st">  // likelihood</span>
<span class="st">  // y ~ normal(mu, sigma);            // previously used normal </span>
<span class="st">  y ~ student_t(10, mu, sigma)         // t with df=10</span>
<span class="st">}</span>
<span class="st">&quot;</span></code></pre></div>
<p>In this case we set the degrees of freedom at 10<label for="tufte-sn-28" class="margin-toggle sidenote-number">28</label><input type="checkbox" id="tufte-sn-28" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">28</span> Alternatively, we could add a value ‘df’ to the data list and data block.</span>, but how would you know in advance what to set it as? It might be better to place a prior (with lower bound of one) for that value and estimate it as part of the modeling process. One should note that there are many distributions available in Stan (e.g. others might be useful for skewed data, truncated etc.), and more will be added in the future.</p>
</div>
<div id="generalized-linear-model" class="section level2">
<h2>Generalized Linear Model</h2>
<p>Expanding from standard linear model, we can move very easily to generalized linear models, of which the standard regression is a special case. The key components are use of a link function that links the linear predictor to the target variable, and an appropriate sampling distribution for the likelihood.</p>
<p>Let’s consider a count model using the Poisson distribution. We can specify the model as follows:</p>
<p><span class="math display">\[y \sim Pois(\lambda)\]</span></p>
<p><span class="math display">\[g(\lambda) = X\beta\]</span></p>
<p>where <span class="math inline">\(g(.)\)</span> is the link function, the canonical link function for Poisson being the natural logarithm. In Stan this can be expressed via the inverse link function, where we exponentiate the linear predictor. Aside from that we simply specify <span class="math inline">\(y\)</span> as distributed Poisson in the same way we used the normal and t-distribution in earlier efforts.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stanmodelcodePoisson =<span class="st"> &quot;</span>
<span class="st">.</span>
<span class="st">.</span>
<span class="st">.</span>

<span class="st">model {                     </span>
<span class="st">  vector[N] lambda;</span>
<span class="st">  vector[N] eta;</span>

<span class="st">  eta &lt;- X * beta;</span>
<span class="st">  lambda &lt;- exp(eta)</span>
<span class="st">   </span>
<span class="st">  // priors</span>
<span class="st">  beta ~ normal(0, 10);</span>

<span class="st">  // likelihood</span>
<span class="st">  y ~ poisson(lambda)</span>
<span class="st">}</span>
<span class="st">&quot;</span></code></pre></div>
<p>And that’s all there is to that<label for="tufte-sn-29" class="margin-toggle sidenote-number">29</label><input type="checkbox" id="tufte-sn-29" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">29</span> Note that some link/inverse-link functions in Stan cannot be applied to vectors, only scalars. As such you would have to loop over the values of <span class="math inline">\(y\)</span>,<br> <code>for(n in 1:N) ...</code></span>. We just saw that we are not limited to the exponential family distributions of glm(s), though that covers a lot of ground, and so at this point you have a lot of the tools covered in standard applied statistics course, and a few beyond.</p>

</div>
</div>
<p style="text-align: center;">
<a href="04_diagnostics.html"><button class="btn btn-default">Previous</button></a>
<a href="06_issues.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
