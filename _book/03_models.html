<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Bayesian Basics" />
<meta property="og:type" content="book" />


<meta property="og:description" content="An introduction to Bayesian data analysis." />
<meta name="github-repo" content="m-clark/docs" />


<meta name="date" content="2016-12-18" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to Bayesian data analysis.">

<title>Bayesian Basics</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="..\css\tufte_bookdown\mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#home"><span style="color:transparent">Home</span></a></li>
<li class="has-sub"><a href="00_preface.html#preface">Preface</a><ul>
<li><a href="00_preface.html#prerequisites">Prerequisites</a></li>
</ul></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li class="has-sub"><a href="01_intro.html#bayesian-probability">Bayesian Probability</a><ul>
<li><a href="01_intro.html#conditional-probability-bayes-theorem">Conditional probability &amp; Bayes theorem</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="02_example.html#a-hands-on-example">A Hands-on Example</a><ul>
<li><a href="02_example.html#prior-likelihood-posterior-distributions">Prior, likelihood, &amp; posterior distributions</a></li>
<li><a href="02_example.html#prior">Prior</a></li>
<li><a href="02_example.html#likelihood">Likelihood</a></li>
<li><a href="02_example.html#posterior">Posterior</a></li>
<li><a href="02_example.html#posterior-predictive">Posterior predictive</a></li>
</ul></li>
<li class="has-sub"><a href="03_models.html#regression-models">Regression Models</a><ul>
<li><a href="03_models.html#example-linear-regression-model">Example: Linear Regression Model</a></li>
<li class="has-sub"><a href="03_models.html#setup">Setup</a><ul>
<li><a href="03_models.html#stan-code">Stan Code</a></li>
<li><a href="03_models.html#running-the-model">Running the Model</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_diagnostics.html#model-checking-diagnostics">Model Checking &amp; Diagnostics</a><ul>
<li class="has-sub"><a href="04_diagnostics.html#monitoring-convergence">Monitoring Convergence</a><ul>
<li><a href="04_diagnostics.html#visual-inspection-traceplot-densities">Visual Inspection: Traceplot &amp; Densities</a></li>
<li><a href="04_diagnostics.html#statistical-measures">Statistical Measures</a></li>
<li><a href="04_diagnostics.html#autocorrelation">Autocorrelation</a></li>
</ul></li>
<li class="has-sub"><a href="04_diagnostics.html#model-checking">Model Checking</a><ul>
<li><a href="04_diagnostics.html#sensitivity-analysis">Sensitivity Analysis</a></li>
<li><a href="04_diagnostics.html#predictive-accuracy-model-comparison">Predictive Accuracy &amp; Model Comparison</a></li>
<li><a href="04_diagnostics.html#posterior-predictive-checking-statistical">Posterior Predictive Checking: Statistical</a></li>
<li><a href="04_diagnostics.html#posterior-predictive-checking-graphical">Posterior Predictive Checking: Graphical</a></li>
</ul></li>
<li><a href="04_diagnostics.html#summary">Summary</a></li>
</ul></li>
<li class="has-sub"><a href="05_enhancements.html#model-enhancements">Model Enhancements</a><ul>
<li><a href="05_enhancements.html#generating-new-variables-of-interest">Generating New Variables of Interest</a></li>
<li><a href="05_enhancements.html#robust-regression">Robust Regression</a></li>
<li><a href="05_enhancements.html#generalized-linear-model">Generalized Linear Model</a></li>
</ul></li>
<li class="has-sub"><a href="06_issues.html#issues">Issues</a><ul>
<li><a href="06_issues.html#debugging">Debugging</a></li>
<li class="has-sub"><a href="06_issues.html#choice-of-prior">Choice of Prior</a><ul>
<li><a href="06_issues.html#noninformative-weakly-informative-informative">Noninformative, Weakly Informative, Informative</a></li>
<li><a href="06_issues.html#conjugacy">Conjugacy</a></li>
<li><a href="06_issues.html#sensitivity-analysis-revisited">Sensitivity Analysis Revisited</a></li>
<li><a href="06_issues.html#summary-1">Summary</a></li>
</ul></li>
<li class="has-sub"><a href="06_issues.html#sampling-procedure">Sampling Procedure</a><ul>
<li><a href="06_issues.html#metropolis">Metropolis</a></li>
<li><a href="06_issues.html#gibbs">Gibbs</a></li>
<li><a href="06_issues.html#hamiltonian-monte-carlo">Hamiltonian Monte Carlo</a></li>
<li><a href="06_issues.html#other-variations-and-approximate-methods">Other Variations and Approximate Methods</a></li>
</ul></li>
<li><a href="06_issues.html#number-of-draws-thinning-warm-up">Number of draws, thinning, warm-up</a></li>
<li><a href="06_issues.html#model-complexity">Model Complexity</a></li>
</ul></li>
<li><a href="1000_Conclusion.html#summary-2">Summary</a></li>
<li class="has-sub"><a href="1001_Appendix.html#appendix">Appendix</a><ul>
<li class="has-sub"><a href="1001_Appendix.html#maximum-likelihood-review">Maximum Likelihood Review</a><ul>
<li><a href="1001_Appendix.html#example">Example</a></li>
</ul></li>
<li><a href="1001_Appendix.html#linear-model">Linear Model</a></li>
<li><a href="1001_Appendix.html#binomial-likelihood-example">Binomial Likelihood Example</a></li>
<li class="has-sub"><a href="1001_Appendix.html#modeling-languages">Modeling Languages</a><ul>
<li><a href="1001_Appendix.html#bugs">Bugs</a></li>
<li><a href="1001_Appendix.html#jags">JAGS</a></li>
<li><a href="1001_Appendix.html#stan">Stan</a></li>
<li><a href="1001_Appendix.html#r">R</a></li>
<li><a href="1001_Appendix.html#general-statistical-packages">General Statistical Packages</a></li>
<li><a href="1001_Appendix.html#other-programming-languages">Other Programming Languages</a></li>
<li><a href="1001_Appendix.html#summary-3">Summary</a></li>
</ul></li>
<li><a href="1001_Appendix.html#bugs-example">BUGS Example</a></li>
<li><a href="1001_Appendix.html#jags-example">JAGS Example</a></li>
<li><a href="1001_Appendix.html#metropolis-hastings-example">Metropolis Hastings Example</a></li>
<li><a href="1001_Appendix.html#hamiltonian-monte-carlo-example">Hamiltonian Monte Carlo Example</a></li>
</ul></li>
<li class="has-sub"><a href="1002_references.html#references">References</a><ul>
<li><a href="1002_references.html#texts">Texts</a></li>
<li><a href="1002_references.html#other">Other</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="regression-models" class="section level1">
<h1>Regression Models</h1>
<p>Now armed with a conceptual understanding of the Bayesian approach, we will actually investigate a regression model using it. To keep things simple, we start with a standard linear model for regression. Later, we will show how easy it can be to add changes to the sampling distribution or priors for alternative modeling techniques. But before getting too far, you should peruse the <a href="1001_Appendix.html#modeling-languages">Modeling Languages</a> section of the appendix to get a sense of some of the programming approaches available. We will be using the programming language Stan via R and the associated R package <span class="pack">rstan</span>. If you prefer to keep things conceptual rather than worry about the code, you can read through the following data description and then skip to <a href="03_models.html#running-the-model">running the model</a>.<span class="marginnote">For just a standard regression model I would ultimately suggest using <span class="pack">rstanarm</span> or <span class="pack">brms</span>, because they would allow you to keep to the usual R approach for modeling, allowing you to more or less jump right in. However, writing Stan code makes it more clear what is being done, so we’ll keep to it for our purposes here. When I first started writing this document, <span class="pack">brms</span> didn’t exist, <span class="pack">rstanarm</span> was still in its infancy, there was no interactive <span class="pack">shinystan</span> etc. I may end up moving the Stan code to the appendix and just using <span class="pack">rstanarm</span> in the future, but I think it’s worth knowing what’s actually going on specifically behind the scenes, so will stick to the <span class="pack">rstan</span> approach for now.</span></p>
<div id="example-linear-regression-model" class="section level2">
<h2>Example: Linear Regression Model</h2>
<p>In the following we will have some initial data set up and also run the model using the standard <span class="func">lm</span> function for later comparison. I choose simulated data so that not only should you know what to expect from the model, it can easily be modified to enable further understanding. I will also use some matrix operations, and if these techniques are unfamiliar to you, you’ll perhaps want to do some refreshing or learning on your own beforehand.</p>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>First we need to create the data we’ll use here and for most of the other examples in this document.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed for replicability</span>
<span class="kw">set.seed</span>(<span class="dv">8675309</span>)

<span class="co"># create a N x k matrix of covariates</span>
N =<span class="st"> </span><span class="dv">250</span>
K =<span class="st"> </span><span class="dv">3</span>

covariates =<span class="st"> </span><span class="kw">replicate</span>(K, <span class="kw">rnorm</span>(<span class="dt">n=</span>N))
<span class="kw">colnames</span>(covariates) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;X1&#39;</span>, <span class="st">&#39;X2&#39;</span>, <span class="st">&#39;X3&#39;</span>)

<span class="co"># create the model matrix with intercept</span>
X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dt">Intercept=</span><span class="dv">1</span>, covariates)

<span class="co"># create a normally distributed variable that is a function of the covariates</span>
coefs =<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, .<span class="dv">2</span>, -<span class="fl">1.5</span>, .<span class="dv">9</span>)
mu =<span class="st"> </span>X %*%<span class="st"> </span>coefs
sigma =<span class="st"> </span><span class="dv">2</span>
y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, mu, sigma)

<span class="co"># same as</span>
<span class="co"># y = 5 + .2*X1 - 1.5*X2 + .9*X3 + rnorm(N, mean=0, sd=2)</span>

<span class="co"># Run lm for later comparison; but go ahead and examine now if desired</span>
modlm =<span class="st"> </span><span class="kw">lm</span>(y~., <span class="dt">data=</span><span class="kw">data.frame</span>(X[,-<span class="dv">1</span>]))
<span class="co"># summary(modlm)</span></code></pre></div>
<p>Just to make sure we’re on the same page, at this point we have three covariates, and a <span class="math inline">\(y\)</span> that is a normally distributed, linear function of them with standard deviation equal to 2. The population values for the coefficients including the intercept are 5, 0.2, -1.5, and 0.9, though with the noise added, the actual estimated values for the sample are slightly different. Now we are ready to set up an R list object of the data for input into Stan, as well as the corresponding Stan code to model this data. I will show all the Stan code, which is implemented in R via a single character string, and then provide some detail on each corresponding model block. However, the goal here isn’t to focus on tools as it is to focus on concepts. <span class="margin_note">Related code for this same model in BUGS and JAGS is provided in the appendix <a href="1001_Appendix.html#bugs-example">here</a>. I don’t think there is an easy way to learn these programming languages except by diving in and using them yourself with models and data you understand.</span></p>
<p>The data list for Stan should include any matrix, vector, or value that might be used in the Stan code. For example, along with the data one can include things like sample size, group indicators (e.g. for mixed models) and so forth. Here we can get by with just the N, the number of columns in the model matrix, the target variable and the model matrix itself.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the data list object for Stan input</span>
dat =<span class="st"> </span><span class="kw">list</span>(<span class="dt">N=</span>N, <span class="dt">K=</span><span class="kw">ncol</span>(X), <span class="dt">y=</span>y, <span class="dt">X=</span>X)</code></pre></div>
<p>Next comes the Stan code. In <span class="pack">R2OpenBugs</span> or <span class="pack">rjags</span> one would call a separate text file with the code, and one can do the same with <span class="pack">rstan</span><label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> In your own Stan pursuits it’s better to have the Stan model as a separate file.</span>, but for our purposes, we’ll display it within the R code. The first thing to note then is the model code. Next, Stan has programming blocks that have to be called in order. I will have all of the blocks in the code to note their order and discuss each in turn, even though we won’t use them all. Anything following a // or #, or between /* */, are comments pertaining to the code. Assignments in Stan are <code>=</code><label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> <code>&lt;-</code> is now deprecated, but you may see older examples using it.</span>, while distributions are specified with a <span class="math inline">\(\sim\)</span>, e.g. <code>y ~ normal(0, 1)</code> means y is normally distributed with mean 0 and standard deviation of 1.</p>
<p>The primary goal here again is to get to the results and beyond, but one should examine the <a href="http://mc-stan.org/documentation/">Stan manual</a> for details about the code. In addition, to install <span class="pack">rstan</span> one will need to do so via CRAN or Github (<a href="https://github.com/Stan-dev/rstan/wiki/RStan-Getting-Started">quickstart guide</a>). It does not require a separate installation of Stan itself, but it does take a couple steps and does require a C++ compiler<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> You can examine this <a href="https://en.wikipedia.org/wiki/List_of_compilers#C.2B.2B_compilers">list</a> of compilers, or on Windows simply install Rtools from the R website (recommended). Note that you may already have one incidentally. Try the Stan test in their ‘getting started’ guide before downloading one.</span>. Once you have <span class="pack">rstan</span> installed it is called like any other R package as will see shortly.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the stan model object using Stan&#39;s syntax</span>
stanmodelcode =<span class="st"> &quot;</span>
<span class="st">data {                      // Data block</span>
<span class="st">  int&lt;lower=1&gt; N;           // Sample size</span>
<span class="st">  int&lt;lower=1&gt; K;           // Dimension of model matrix</span>
<span class="st">  matrix[N, K] X;           // Model Matrix</span>
<span class="st">  vector[N] y;              // Target variable</span>
<span class="st">}</span>

<span class="st">/* </span>
<span class="st">transformed data {          // Transformed data block. Not used presently.</span>
<span class="st">} </span>
<span class="st">*/</span>

<span class="st">parameters {                // Parameters block</span>
<span class="st">  vector[K] beta;           // Coefficient vector</span>
<span class="st">  real&lt;lower=0&gt; sigma;      // Error scale</span>
<span class="st">}</span>

<span class="st">model {                     // Model block</span>
<span class="st">  vector[N] mu;</span>
<span class="st">  mu &lt;- X * beta;           // Creation of linear predictor</span>
<span class="st">  </span>
<span class="st">  // priors</span>
<span class="st">  beta ~ normal(0, 10);</span>
<span class="st">  sigma ~ cauchy(0, 5);     // With sigma bounded at 0, this is half-cauchy</span>
<span class="st">  </span>
<span class="st">  // likelihood</span>
<span class="st">  y ~ normal(mu, sigma);</span>
<span class="st">}</span>

<span class="st">/*</span>
<span class="st">generated quantities {      // Generated quantities block. Not used presently.</span>
<span class="st">}</span>
<span class="st">*/</span>
<span class="st">&quot;</span></code></pre></div>
<div id="stan-code" class="section level3">
<h3>Stan Code</h3>
<p>The first section is the <span class="emph">data</span> block, where we tell Stan the data it should be expecting from the data list. It is useful to put in bounds as a check on the data input, and that is what is being done between the <code>&lt; &gt;</code> (e.g. we should at least have a sample size of 1). The first two variables declared are <code>N</code> and <code>K</code>, both as integers. Next the code declares the model matrix and target vector respectively. As you’ll note here and for the next blocks, we declare the type and dimensions of the variable and then its name. In Stan, everything declared in one block is available to subsequent blocks, but those declared in a block may not be used in earlier blocks. Even within a block, anything declared, such as <code>N</code> and <code>K</code>, can then be used subsequently, as we did to specify dimensions of the model matrix <code>X</code>.</p>
<p>For a reference, the following is from the Stan manual, and notes variables of interest and the associated blocks where they would be declared.</p>
<table class="gmisc_table" style="border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;">
<thead>
<tr>
<th style="border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
Variable Kind
</th>
<th style="border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
Declaration Block
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">
modeled, unmodeled data
</td>
<td style="text-align: center;">
data, transformed data
</td>
</tr>
<tr>
<td style="text-align: center;">
modeled parameters, missing data
</td>
<td style="text-align: center;">
parameters, transformed parameters
</td>
</tr>
<tr>
<td style="text-align: center;">
unmodeled parameters
</td>
<td style="text-align: center;">
data, transformed data
</td>
</tr>
<tr>
<td style="text-align: center;">
generated quantities
</td>
<td style="text-align: center;">
transformed data, transformed parameters, generated quantities
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid grey; text-align: center;">
loop indices
</td>
<td style="border-bottom: 2px solid grey; text-align: center;">
loop statement
</td>
</tr>
</tbody>
</table>
<p>The <span class="emph">transformed data</span> block is where you could do such things as log or center variables and similar, i.e. you can create new data based on the input data or just in general. If you are using R though, it would almost always be easier to do those things in R first and just include them in the data list. You can also declare any unmodeled parameters here, e.g. those you want fixed at some value.</p>
<p>The primary parameters of interest that are to be estimated go in the <span class="emph">parameters </span> block. As with the data block you can only declare these variables, you cannot make any assignments. Here we note the <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> to be estimated, with a lower bound of zero on the latter. In practice you might prefer to split out the intercept or other coefficients to be modeled separately if they are on notably different scales.</p>
<p>The <span class="emph">transformed parameters</span> block is where optional parameters of interest might be included. What might go here is fairly open, but for efficiency’s sake you will typically want to put things only of specific interest that are dependent on the parameters block. These are evaluated along with the parameters, so if the objects are not of special interest you can instead generate them in the model or generated quantities block to save time.</p>
<p>The <span class="emph">model</span> block is where your priors and likelihood are specified, along with the declaration of any variables necessary. As an example, the linear predictor is included here, as it will go towards the likelihood<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> The position within the model block isn’t crucial. I tend to like to do all the variable declarations at the start, but others might prefer to have them under the likelihood heading at the point they are actually used.</span>. Note that we could have instead put the linear predictor in the transformed parameters section, but this would slow down the process, and again, we’re not so interested in those specific values.</p>
<p>I use a normal prior for the coefficients with a zero mean and a very large standard deviation to reflect my notable ignorance here<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> By setting the prior mean to zero, this will have the effect of shrinking the coefficients toward zero to some extent. In this sense, it is equivalent to penalized regression in the non-Bayesian setting, ridge regression in particular.</span>. For the <span class="math inline">\(\sigma\)</span> estimate I use a Cauchy distribution<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> Actually a half-Cauchy as it is bounded to be positive. This is equivalent to a student t with df=1, and there is some tendency of late to use the student t directly with df=3 for slight gains in performance for some models.</span>. Many regression examples using BUGS will use an inverse gamma prior, which is perfectly okay for this model, though it would not work so well for other variance parameters. Had we not specified anything for the prior distribution for the parameters, vague (discussed more in the <a href="06_issues.html#choice-of-prior">Choice of Prior section</a>), uniform distributions would be the default. The likelihood is specified in a similar manner as one would with R. BUGS style languages would actually use <span class="func">dnorm</span> as in R, though Stan uses <span class="func">normal</span> for the function name.</p>
<p>Finally, we get to the <span class="emph">generated quantities</span>, which is kind of a fun zone. <em>Anything</em> you want to calculate can go here - predictions on new data, ratios of parameters, how many times a parameter is greater than x, transformations of parameters for reporting purposes, and so forth. We will demonstrate this later.</p>
</div>
<div id="running-the-model" class="section level3">
<h3>Running the Model</h3>
<p>Now that we have an idea of what the code is doing, let’s put it to work. Bayesian estimation, like maximum likelihood, starts with initial guesses as starting points and then runs in an iterative fashion, producing simulated draws from the posterior distribution at each step, and then correcting those draws until finally getting to some target, or <span class="emph">stationary</span> distribution. This part is key and different from classical statistics. We are aiming for a <em>distribution</em>, not a <em>point estimate</em>.</p>
<p>The simulation process is referred to as <span class="emph">Markov Chain Monte Carlo</span>, or MCMC for short. The specifics of this process are what sets many of the Bayesian programming languages/approaches apart, and something we will cover in more detail in a later section (see <a href="06_issues.html#sampling-procedure">Sampling Procedure</a>). In MCMC, all of the simulated draws from the posterior are based on and correlated with previous draws<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> In a Markov Chain, <span class="math inline">\(\theta_t\)</span> is independent of previous <span class="math inline">\(\theta_{t-2...t_1}\)</span>, conditional on <span class="math inline">\(\theta_{t-1}\)</span>.</span>, as the process moves along the path toward a stationary distribution. Typically we will allow the process to <em>warm up</em>, or rather get a bit settled down from the initial starting point, which might be way off, and thus the subsequent estimates will also be way off for the first few iterations. <span class="marginnote">How far one wants to go down the rabbit hole regarding MCMC is up to the reader. A great many applied researchers do classical statistical analysis without putting much thought into the actual maximum likelihood estimation process, and I suppose one could do so here as well.</span> Rest assured, assuming the model and data are otherwise acceptable, the process will get to where it needs to go. However, as a further check, we will run the whole thing multiple times, i.e. have more than one <span class="emph">chain</span>. As the chains will start from different places, if multiple chains get to the same place in the end, we can feel more confident about our results.</p>
<p>While this process may sound like it might take a long time to complete, for the following you’ll note that it will likely take more time for Stan to compile its code to C++ than it will to run the model<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> Not usually the case except for simple models with smaller data.</span>, and on my computer each chain only takes only a little more than a second. However, the Bayesian approach used to take a very long time even for a standard regression such as this, and that is perhaps the primary reason why Bayesian analysis only caught on in the last couple decades; we simply didn’t have the machines to do it efficiently. Even now though, for highly complex models and large data sets it can still take a long time to run, though typically not prohibitively so.</p>
<p>In the following code, we note the object that contains the Stan model code, the data list, how many iterations we want (12000)<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> This is way overkill for a simple model like this. One probably would be fine with 500 warmup and 500 iterations for a standard regression.</span>, how long we want the process to run before we start to keep any estimates (<code>warmup=2000</code>), how many of the post-warmup draws of the posterior we want to keep (<code>thin=10</code> means every tenth draw), and the number of chains (<code>chains=3</code>). In the end we will have three chains of 1000<span class="marginnote"><span class="math inline">\(\frac{12000-2000}{10} = 1000\)</span></span> draws from the posterior distribution of the parameters. Stan spits out a lot of output to the R console even with <code>verbose = FALSE</code>, and I omit it here, but you will see some initial info about the compiling process, updates as each chain gets through 10% of iterations specified in the <code>iter</code> argument, and finally an estimate of the elapsed time. You may also see <em>informational messages</em> which, unless they are highly repetitive, should not be taken as an error.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rstan)

### Run the model and examine results ###
fit =<span class="st"> </span><span class="kw">stan</span>(<span class="dt">model_code=</span>stanmodelcode, <span class="dt">data=</span>dat, <span class="dt">iter=</span><span class="dv">12000</span>, 
           <span class="dt">warmup=</span><span class="dv">2000</span>, <span class="dt">thin=</span><span class="dv">10</span>, <span class="dt">chains=</span><span class="dv">3</span>)</code></pre></div>
<p>With the model run, we can now examine the results. In the following, we specify the digit precision to display, which parameters we want (not necessary here), and which quantiles of the posterior draws we want, which in this case are the median and those that would produce a 95% interval estimate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># summary</span>
<span class="kw">print</span>(fit, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&#39;beta&#39;</span>, <span class="st">&#39;sigma&#39;</span>), <span class="dt">digits=</span><span class="dv">3</span>, <span class="dt">prob=</span><span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">5</span>,.<span class="dv">975</span>))</code></pre></div>
<pre><code>Inference for Stan model: 2c7fc0327865079c07fd921e7cbb2b0a.
3 chains, each with iter=12000; warmup=2000; thin=10; 
post-warmup draws per chain=1000, total post-warmup draws=3000.

          mean se_mean    sd   2.5%    50%  97.5% n_eff  Rhat
beta[1]  4.895   0.002 0.130  4.637  4.896  5.142  2951 1.000
beta[2]  0.083   0.002 0.133 -0.183  0.084  0.341  3000 1.001
beta[3] -1.467   0.002 0.127 -1.711 -1.469 -1.215  3000 1.000
beta[4]  0.822   0.002 0.122  0.582  0.824  1.060  2862 1.000
sigma    2.030   0.002 0.094  1.859  2.026  2.224  2886 1.000

Samples were drawn using NUTS(diag_e) at Sat May 07 12:00:28 2016.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>So far so good. The mean estimates reflect the mean of posterior draws for the parameters of interest, and are the typical coefficients reported in standard regression analysis. The 95% probability, or, <span class="emph">credible intervals</span> are worth noting, because <em>they are not confidence intervals as you know them</em>. There is no repeated sampling interpretation here<label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> A standard confidence implies that if we’d done the study exactly the same over and over, and calculated a confidence interval each time, 95% of them would capture the true value. The one you have is just one from that process.</span>. The probability interval is more intuitive. It means simply that, based on the results of this model, there is a 95% chance the true value will fall between those two points. The other values printed out I will return to in just a moment.</p>
<p>Comparing the results to those from R’s <span class="func">lm</span> function, we can see we obtain similar estimates, as they are identical to two decimal places. In fact, had we used uniform priors<label for="tufte-sn-18" class="margin-toggle sidenote-number">18</label><input type="checkbox" id="tufte-sn-18" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">18</span> In Stan code this can be done by not explicitly specifying a prior.</span>, we would doing essentially the same model as what is being conducted with standard maximum likelihood estimation. Here, we have a decent amount of data for a model that isn’t complex, so we would expect the likelihood to notably outweigh the prior, as we demonstrated previously with our binomial example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(modlm)</code></pre></div>
<pre><code>
Call:
lm(formula = y ~ ., data = data.frame(X[, -1]))

Residuals:
    Min      1Q  Median      3Q     Max 
-6.8632 -1.4696  0.2431  1.4213  5.0406 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.89777    0.12845  38.131  &lt; 2e-16 ***
X1           0.08408    0.12960   0.649    0.517    
X2          -1.46861    0.12615 -11.642  &lt; 2e-16 ***
X3           0.81959    0.12065   6.793 8.21e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 2.021 on 246 degrees of freedom
Multiple R-squared:  0.4524,    Adjusted R-squared:  0.4458 
F-statistic: 67.75 on 3 and 246 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>But how would we know if our model was working out okay otherwise? There are several standard diagnostics, and we will talk about them in more detail in the next section, but let’s take a look at some presently. In the summary, <code>se_mean</code> is the <span class="emph">Monte Carlo error</span>, and is an estimate of the uncertainty contributed by only having a finite number of posterior draws. <code>n_eff</code> is <span class="emph">effective sample size</span> given all chains, and essentially accounts for autocorrelation in the chain, i.e. the correlation of the estimates as we go from one draw to the next. It actually doesn’t have to be very large, but if it was small relative to the total number of draws desired that might be cause for concern. <code>Rhat</code> is a measure of how well chains mix, and goes to 1 as chains are allowed to run for an infinite number of draws. In this case, <code>n_eff</code> and <code>Rhat</code> suggest we have good convergence, but we can also examine this visually with a traceplot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Visualize</span>
<span class="kw">stan_trace</span>(fit, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&#39;beta[4]&#39;</span>))</code></pre></div>
<p><span class="imgbigger"><img src="img/traceplotWarmup.svg" width=75% style="display:block; margin: 0 auto;"></span></p>
<p>I only show one parameter for the current demonstration, but one should always look at the traceplots for all parameters. What we are looking for after the warmup period is a “fat hairy caterpillar” or something that might be labeled as “grassy”, and this plot qualifies as such<label for="tufte-sn-19" class="margin-toggle sidenote-number">19</label><input type="checkbox" id="tufte-sn-19" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">19</span> Like all model diagnostics, we aren’t dealing with an exact science.</span>. One can see that the estimates from each chain find their way from the starting point to a more or less steady state quite rapidly (initial warmup iterations in gray). Furthermore, all three chains, each noted by a different color, are mixing well and bouncing around the same conclusion. The statistical measures and traceplot suggest that we are doing okay.</p>
<p>The Stan development crew has made it easy to interactively explore diagnostics via the <span class="pack">shinystan</span> package, and one should do so with each model. In addition, there are other diagnostics available in the <span class="pack">coda</span> package, and Stan model results can be easily converted to work with it. The following code demonstrates how to get started.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(coda)
betas =<span class="st"> </span><span class="kw">extract</span>(fit, <span class="dt">pars=</span><span class="st">&#39;beta&#39;</span>)$beta
betas.mcmc =<span class="st"> </span><span class="kw">as.mcmc</span>(betas)
<span class="kw">plot</span>(betas.mcmc)</code></pre></div>
<p>So there you have it. Aside from the initial setup with making a data list and producing the language-specific model code, it doesn’t necessarily take much to running a Bayesian regression model relative to standard models<label for="tufte-sn-20" class="margin-toggle sidenote-number">20</label><input type="checkbox" id="tufte-sn-20" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">20</span> As noted previously, other R packages would allow for regression models to be specified just like you would with the <span class="func">lm</span> and <span class="func">glm</span> functions. See the <span class="pack">rstanarm</span> (from the developers of Stan) and <span class="pack">brms</span> packages especially.</span>. The main thing perhaps is simply employing a different mindset, and interpreting the results from within that new perspective. For the standard models you are familiar with, it probably won’t take too long to be as comfortable here as you were with those, and now you will have a flexible tool to take you into new realms with deeper understanding.</p>

</div>
</div>
</div>
<p style="text-align: center;">
<a href="02_example.html"><button class="btn btn-default">Previous</button></a>
<a href="04_diagnostics.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
