<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Chapter 3 A Hands-on Example | Bayesian Basics</title>

  
   <meta name="description" content="This document provides an introduction to Bayesian data analysis. It is conceptual in nature, but uses the probabilistic programming language Stan for demonstration (and its implementation in R via rstan). From elementary examples, guidance is provided for data preparation, efficient modeling, diagnostics, and more." />
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="Chapter 3 A Hands-on Example | Bayesian Basics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://m-clark.github.io/bayesian-basics/" />
  <meta property="og:image" content="https://m-clark.github.io/bayesian-basics//img/nineteeneightyR.png" />
  <meta property="og:description" content="This document provides an introduction to Bayesian data analysis. It is conceptual in nature, but uses the probabilistic programming language Stan for demonstration (and its implementation in R via rstan). From elementary examples, guidance is provided for data preparation, efficient modeling, diagnostics, and more." />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 A Hands-on Example | Bayesian Basics" />
  
  <meta name="twitter:description" content="This document provides an introduction to Bayesian data analysis. It is conceptual in nature, but uses the probabilistic programming language Stan for demonstration (and its implementation in R via rstan). From elementary examples, guidance is provided for data preparation, efficient modeling, diagnostics, and more." />
  <meta name="twitter:image" content="https://m-clark.github.io/bayesian-basics//img/nineteeneightyR.png" />
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.3.1/transition.js"></script>
    <script src="libs/bs3compat-0.3.1/tabs.js"></script>
    <script src="libs/bs3compat-0.3.1/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
    <script src="libs/plotly-binding-4.10.0/plotly.js"></script>
    <script src="libs/typedarray-0.1/typedarray.min.js"></script>
    <link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
    <script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
    <link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
    <script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
  
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Bayesian Basics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="a-hands-on-example" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> A Hands-on Example</h1>
<div id="prior-likelihood-posterior-distributions" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Prior, likelihood, &amp; posterior distributions</h2>
<p>The following is an attempt to provide a small example to show the connection between the prior distribution, likelihood, and posterior distribution. Let’s say we want to estimate the probability that a soccer/football player<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> will score a penalty kick in a shootout. We will employ the binomial distribution to model this.</p>
<p>Our goal is to estimate a parameter <span class="math inline">\(\theta\)</span>, the probability that the random knucklehead from your favorite football team will score the penalty in an overtime shootout. Let’s say that for this match it takes 10 shots per team before the game is decided.</p>
<p>In R, we can represent the following data for your team as follows, as well as set up some other things for later.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="a-hands-on-example.html#cb1-1" aria-hidden="true" tabindex="-1"></a>shots <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&#39;goal&#39;</span>,<span class="st">&#39;goal&#39;</span>,<span class="st">&#39;goal&#39;</span>,<span class="st">&#39;miss&#39;</span>,<span class="st">&#39;miss&#39;</span>,</span>
<span id="cb1-2"><a href="a-hands-on-example.html#cb1-2" aria-hidden="true" tabindex="-1"></a>          <span class="st">&#39;goal&#39;</span>,<span class="st">&#39;goal&#39;</span>,<span class="st">&#39;miss&#39;</span>,<span class="st">&#39;goal&#39;</span>,<span class="st">&#39;goal&#39;</span>)</span>
<span id="cb1-3"><a href="a-hands-on-example.html#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="a-hands-on-example.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to numeric, arbitrarily picking goal=1, miss=0</span></span>
<span id="cb1-5"><a href="a-hands-on-example.html#cb1-5" aria-hidden="true" tabindex="-1"></a>shotsNum <span class="ot">=</span> <span class="fu">as.numeric</span>(shots <span class="sc">==</span> <span class="st">&#39;goal&#39;</span>)</span>
<span id="cb1-6"><a href="a-hands-on-example.html#cb1-6" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="fu">length</span>(shots)                      <span class="co"># sample size</span></span>
<span id="cb1-7"><a href="a-hands-on-example.html#cb1-7" aria-hidden="true" tabindex="-1"></a>nGoal <span class="ot">=</span> <span class="fu">sum</span>(shots <span class="sc">==</span> <span class="st">&#39;goal&#39;</span>)           <span class="co"># number of shots made</span></span>
<span id="cb1-8"><a href="a-hands-on-example.html#cb1-8" aria-hidden="true" tabindex="-1"></a>nMiss <span class="ot">=</span> <span class="fu">sum</span>(shots <span class="sc">==</span> <span class="st">&#39;miss&#39;</span>)           <span class="co"># number of those miss</span></span></code></pre></div>
<p>Recall the binomial distribution, where we specify the number of trials for a particular observation, and the probability of an event’s occurrence. Let’s look at the distribution for a couple values for <span class="math inline">\(\theta\)</span> equal to .5 and .85, and <span class="math inline">\(N = 10\)</span> observations. We will repeat this 1000 times.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="a-hands-on-example.html#cb2-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="dv">1000</span>, <span class="at">size =</span> <span class="dv">10</span>, <span class="at">p =</span> .<span class="dv">5</span>)</span>
<span id="cb2-2"><a href="a-hands-on-example.html#cb2-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="dv">1000</span>, <span class="at">size =</span> <span class="dv">10</span>, <span class="at">p =</span> .<span class="dv">85</span>)</span>
<span id="cb2-3"><a href="a-hands-on-example.html#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="a-hands-on-example.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(x1); <span class="fu">hist</span>(x1)</span></code></pre></div>
<pre><code>[1] 5.043</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="a-hands-on-example.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(x2); <span class="fu">hist</span>(x2)</span></code></pre></div>
<pre><code>[1] 8.569</code></pre>
<p>Though I invite you to inspect them, the histograms are not shown, but we can see above that the means are roughly around <span class="math inline">\(N*p\)</span> as we expect with the binomial.</p>
</div>
<div id="prior" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Prior</h2>
<p>For our current situation, we don’t know <span class="math inline">\(\theta\)</span> and are trying to estimate it. We will start by supplying some possible values. To keep things simple, we’ll only consider 10 values that fall between 0 and 1.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="a-hands-on-example.html#cb6-1" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">1</span> <span class="sc">/</span> (N <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb6-2"><a href="a-hands-on-example.html#cb6-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">to =</span> N <span class="sc">/</span> (N <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb6-3"><a href="a-hands-on-example.html#cb6-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">length =</span> <span class="dv">10</span>)</span>
<span id="cb6-4"><a href="a-hands-on-example.html#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="a-hands-on-example.html#cb6-5" aria-hidden="true" tabindex="-1"></a>theta</span></code></pre></div>
<pre><code> [1] 0.09090909 0.18181818 0.27272727 0.36363636 0.45454545 0.54545455 0.63636364 0.72727273 0.81818182 0.90909091</code></pre>
<p>For the Bayesian approach we must choose a <em>prior distribution</em> representing our initial beliefs about the estimates we might potentially consider. I provide three possibilities, and note that any one of them would work just fine for this situation. We’ll go with a triangular distribution, which will put most of the weight toward values around <span class="math inline">\(.5\)</span>. While we will talk more about this later, I will go ahead and mention now that this is where some people have specifically taken issue with Bayesian estimation in the past, because this part of the process is too <em>subjective</em> for their tastes. Setting aside the fact that subjectivity is an inherent part of the scientific process, and that ignoring prior information (if explicitly available from prior research) would be blatantly unscientific, the main point to make here is that this choice <em>is not an arbitrary one</em>. There are many distributions we might work with, but some will be better for us than others. Again, we’ll revisit this topic later. While we will only work with one prior, I provide others you can play with<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="a-hands-on-example.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="do">### prior distribution</span></span>
<span id="cb8-2"><a href="a-hands-on-example.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># triangular as in Kruschke text example</span></span>
<span id="cb8-3"><a href="a-hands-on-example.html#cb8-3" aria-hidden="true" tabindex="-1"></a>pTheta <span class="ot">=</span> <span class="fu">pmin</span>(theta, <span class="dv">1</span> <span class="sc">-</span> theta)</span>
<span id="cb8-4"><a href="a-hands-on-example.html#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="a-hands-on-example.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># uniform</span></span>
<span id="cb8-6"><a href="a-hands-on-example.html#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># pTheta = dunif(theta)</span></span>
<span id="cb8-7"><a href="a-hands-on-example.html#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="a-hands-on-example.html#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># beta prior with mean = .5</span></span>
<span id="cb8-9"><a href="a-hands-on-example.html#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># pTheta = dbeta(theta, 10, 10)</span></span>
<span id="cb8-10"><a href="a-hands-on-example.html#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="a-hands-on-example.html#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize so that values sum to 1</span></span>
<span id="cb8-12"><a href="a-hands-on-example.html#cb8-12" aria-hidden="true" tabindex="-1"></a>pTheta <span class="ot">=</span> pTheta <span class="sc">/</span> <span class="fu">sum</span>(pTheta) </span></code></pre></div>
<p>So, given some estimate of <span class="math inline">\(\theta\)</span>, we have a probability of that value based on our chosen prior.</p>
</div>
<div id="likelihood" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Likelihood</h2>
<p>Next we will compute the <em>likelihood</em> of the data given some value of <span class="math inline">\(\theta\)</span>. Generally, the likelihood for some target variable <span class="math inline">\(y\)</span>, with observed values <span class="math inline">\(i \dots n\)</span>, given some (set of) parameter(s) <span class="math inline">\(\theta\)</span>, can be expressed as follows:</p>
<p><span class="math display">\[p(y|\theta) = \prod_{i}^{n} p(y_i|\theta)\]</span></p>
<p>Specifically, the likelihood function for the binomial can be expressed as:</p>
<p><span class="math display">\[p(y|\theta) = {N \choose k}\, \theta^k\,  (1-\theta)^{N-k}\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the total number of possible times in which the event of interest could occur, and <span class="math inline">\(k\)</span> number of times the event of interest occurs. Our maximum likelihood estimate in this simple setting would simply be the proportion of events witnessed out of the total number of samples<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>. We’ll use the formula presented above. Technically, the first term is not required, but it serves to normalize the likelihood as we did with the prior<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="a-hands-on-example.html#cb9-1" aria-hidden="true" tabindex="-1"></a>pDataGivenTheta <span class="ot">=</span> <span class="fu">choose</span>(N, nGoal) <span class="sc">*</span> theta<span class="sc">^</span>nGoal <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span>nMiss</span></code></pre></div>
</div>
<div id="posterior" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Posterior</h2>
<p>Given the prior and likelihood, we can now compute the <em>posterior distribution</em> via Bayes theorem. The only thing left to calculate is the denominator from Bayes theorem, then plug in the rest.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="a-hands-on-example.html#cb10-1" aria-hidden="true" tabindex="-1"></a>pData <span class="ot">=</span> <span class="fu">sum</span>(pDataGivenTheta <span class="sc">*</span> pTheta)             <span class="co"># marginal probability of the data</span></span>
<span id="cb10-2"><a href="a-hands-on-example.html#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="a-hands-on-example.html#cb10-3" aria-hidden="true" tabindex="-1"></a>pThetaGivenData <span class="ot">=</span> pDataGivenTheta<span class="sc">*</span>pTheta <span class="sc">/</span> pData  <span class="co"># Bayes theorem</span></span></code></pre></div>
<p>Now let’s examine what all we’ve got.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
theta
</th>
<th style="text-align:center;">
prior
</th>
<th style="text-align:center;">
likelihood
</th>
<th style="text-align:center;">
posterior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.091
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.033
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.000
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.000
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.182
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.067
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.000
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.000
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.273
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.100
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.005
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.004
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.364
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.133
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.026
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.030
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.455
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.167
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.078
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.112
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.545
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.167
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.162
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.232
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.636
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.133
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.244
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.280
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.727
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.100
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.262
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.226
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.818
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.067
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.177
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.102
</td>
</tr>
<tr>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.909
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.033
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.046
</td>
<td style="text-align:center;width: 100px; font-family: monospace;font-size: 90%">
0.013
</td>
</tr>
</tbody>
</table>
<p>Starting with the <em>prior</em> column, we can see that with the triangular distribution, we’ve given most of our prior probability to the middle values with probability tapering off somewhat slowly towards either extreme. The likelihood, on the other hand, suggests the data is most likely for <span class="math inline">\(\theta\)</span> values .64-.73, though we know the specific maximum likelihood estimate for <span class="math inline">\(\theta\)</span> is the proportion for the sample, or 0.7. Our posterior estimate will therefore fall somewhere between the prior and likelihood estimates, and we can see it has shifted the bulk of the probability slightly away from the most likely values suggested by the prior distribution, and towards a <span class="math inline">\(\theta\)</span> value suggested by the data of 0.7.</p>
<p>Let’s go ahead and see what the mean<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> is:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="a-hands-on-example.html#cb11-1" aria-hidden="true" tabindex="-1"></a>posteriorMean <span class="ot">=</span> <span class="fu">sum</span>(pThetaGivenData <span class="sc">*</span> theta)</span>
<span id="cb11-2"><a href="a-hands-on-example.html#cb11-2" aria-hidden="true" tabindex="-1"></a>posteriorMean</span></code></pre></div>
<pre><code>[1] 0.6275941</code></pre>
<p>So, we start with a prior centered on a value of <span class="math inline">\(\theta=.5\)</span>, add data whose ML estimate is <span class="math inline">\(\theta=.7\)</span>, and our posterior distribution suggests we end up somewhere in between.</p>
<p>We can perhaps understand this further via the following visualizations. In each of these the prior is represented by the blue density, the likelihood by the red, and the posterior by purple. This first is based on a different prior than just used in our example, and instead employs the beta distribution noted among the possibilities in the <a href="a-hands-on-example.html#prior">code above</a>. The beta distribution is highly flexible, and with shape parameters <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{B}\)</span> set to 10 and 10 we get a symmetric distribution centered on <span class="math inline">\(\theta = .5\)</span>. This would actually be a somewhat stronger prior than we might normally want to use, but serves to illustrate a point. The mean of the beta is <span class="math inline">\(\frac{\mathcal{A}}{\mathcal{A}+\mathcal{B}}\)</span>, and thus has a nice interpretation as a prior based on data with sample size equal to <span class="math inline">\(\mathcal{A}+\mathcal{B}\)</span>. The posterior distribution that results would have a mean somewhere between the maximum likelihood value and that of the prior. With the stronger prior, the posterior is pulled closer to it.</p>
<p><img src="Bayesian-Basics_files/figure-html/prior2post_1-1.svg" width="672" style="display: block; margin: auto;" /></p>
<p>The second utilizes a more diffuse prior of <span class="math inline">\(\beta(2,2)\)</span><a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>. The result of using the vague prior is that the likelihood gets more weight with regard to the posterior. In fact, if we used a uniform distribution, <em>we would essentially be doing the equivalent of maximum likelihood estimation</em>. In that sense, many of the commonly used methods that implement maximum likelihood can be seen as a special case of a Bayesian approach.</p>
<p><img src="Bayesian-Basics_files/figure-html/prior2post_2-1.svg" width="672" style="display: block; margin: auto;" /></p>
<p>The third graph employs the initial <span class="math inline">\(\beta(10,10)\)</span> prior again, but this time we add more observations to the data. This serves to give more weight to the likelihood, which is what we want. As scientists, we’d want the evidence, i.e. data, to eventually outweigh our prior beliefs about the state of things the more we have of it.</p>
<p><img src="Bayesian-Basics_files/figure-html/prior2post_3-1.svg" width="672" style="display: block; margin: auto;" /></p>
<p>For an interactive demonstration of the above, <a href="https://micl.shinyapps.io/prior2post/">see this</a>.</p>
</div>
<div id="posterior-predictive" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Posterior predictive</h2>
<p>At this point it is hoped you have a better understanding of the process of Bayesian estimation. Conceptually, one starts with prior beliefs about the state of the world and adds evidence to one’s understanding, ultimately coming to a conclusion that serves as a combination of evidence and prior belief. More concretely, we have a prior distribution regarding parameters, a distribution regarding the data given those parameters, and finally a posterior distribution that is the weighted combination of the two.</p>
<p>However, there is yet another distribution of interest to us- the <em>posterior predictive distribution</em>. Stated simply, once we have the posterior distribution for <span class="math inline">\(\theta\)</span>, we can then feed (possibly new or unobserved) data into the data generating process, and get distributions for <span class="math inline">\(\tilde{y}\)</span><a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>. Where <span class="math inline">\(\tilde{y}\)</span> can regard any potential observation, we can distinguish it from the case where we use the current data to produce <span class="math inline">\(y^{\textrm{Rep}}\)</span>, i.e. a replicate of <span class="math inline">\(y\)</span>. For example, if a regression model had predictor variables <span class="math inline">\(X\)</span>, the predictor variables are identical for producing <span class="math inline">\(y^{\textrm{Rep}}\)</span> as they were in modeling <span class="math inline">\(y\)</span>. However, <span class="math inline">\(\tilde{y}\)</span> might be based on any values <span class="math inline">\(\tilde{X}\)</span> that might be feasible or interesting, whether actually observed in the data or not. Since <span class="math inline">\(y^{\textrm{Rep}}\)</span> is an attempt to replicate the observed data based on the parameters <span class="math inline">\(\theta\)</span>, we can compare our simulated data to the observed data to see how well they match.</p>
<p>We can implement the simulation process with the data and model at hand, given a sample of values of <span class="math inline">\(\theta\)</span> drawn from the posterior. I provide the results of such a process with the following graph. Each bar graph of frequencies represents a replication of the 10 shots taken, i.e. <span class="math inline">\(y^{\textrm{Rep}}\)</span>, given an estimate of <span class="math inline">\(\theta\)</span> from the posterior distribution (16 total). These are plausible sets of 10 makes and misses, given <span class="math inline">\(\theta\)</span>.</p>
<p><img src="img/histofyrepBinom.svg" style="display:block; margin: 0 auto;" width=66%></span></p>
<p><br></p>
<p>With an understanding of the key elements of Bayesian inference in hand, we can proceed to the still familiar but more complex and interesting setting of a regression model.</p>
<!-- 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Prior likelihood posterior triplots for posterity
 -->

</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>Don’t even start, it’s always been both ‘football’ and ‘soccer.’<a href="a-hands-on-example.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Choose the prior that makes most sense to you. If I were thinking logically, I might choose a prior that reflects that all advantage is to a person that spends their entire life kicking a ball, rather than the person who is required by the rules to stand still. If the penalty taker can simply kick to the upper right or left of the goal, even with only moderate pace, they could point it out like Babe Ruth every time, and simple physics, i.e. that the goalie can never reach the ball given that he has to start in the middle, would mean they score 95% of the time under perfect conditions. We might revise it downward to account for unspecified things, e.g. how tired they are, weather, etc. A prior based on a beta distribution <code>beta(9,1)</code>, with a mean of .90, would perhaps be about right. Of course, the <em>data</em> would eventually suggest something much lower, perhaps because soccer players aren’t that bright and don’t listen to their team statistician, or perhaps the games are rigged, but it’s chalked up to drama and mind games because that’s what the fans want to believe (and, more importantly, will pay for). I’ll let you decide that binomial outcome.<a href="a-hands-on-example.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>See for yourself in the <a href="appendix.html#binomial-likelihood-example">binomial likelihood</a> section in the appendix.<a href="a-hands-on-example.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Note that if we had covariates as in a regression model, we would have different estimates of theta for each observation, and thus would calculate each observation’s likelihood and then take their product or sum their log values, see the <a href="appendix.html#maximum-likelihood-review">Maximum Likelihhood Review</a> for further details). Even here, if you turn this into binary logistic regression with 10 outcomes of goal scored vs. not, the ‘intercept only’ model would be identical to our results here.<a href="a-hands-on-example.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>The expected value for a continuous parameter is <span class="math inline">\(\operatorname{E}[X] = \int_{-\infty}^\infty xp(x)\mathrm{d}x\)</span>, and for a discrete parameter <span class="math inline">\(\operatorname{E}[X] = \sum_{i=1}^\infty x_i\, p_i\)</span>, i.e. a weighted sum of the possible values times their respective probability of occurrence.<a href="a-hands-on-example.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p><span class="math inline">\(\beta(1,1)\)</span> is a uniform distribution.<a href="a-hands-on-example.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>Mathematically represented as: <span class="math inline">\(p(\tilde{y}|y) = \int p(\tilde{y}|\theta)p(\theta|y)\mathrm{d}\theta\)</span> <br><br> We can get a sense of the structure of this process via the following table, taken from Gelman: <br><br> <img class='zoom' src="img/gelmanTable.png" style="display:block; margin: 0 auto;" width='50%'><a href="a-hands-on-example.html#fnref14" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Bayesian Basics</strong>" was written by <p><span class="noem">Michael Clark</span> <br>
<a href="https://m-clark.github.io/">m-clark.github.io</a></p>. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
